{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone\n",
    "!pip install langchain_huggingface\n",
    "!pip install langchain_groq\n",
    "!pip install langgraph\n",
    "!pip install langchain_experimental\n",
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T73lQj2Ifnw0"
   },
   "source": [
    "# Scrape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "class BigBangTranscriptScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://bigbangtrans.wordpress.com/\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "        # Season 1 episodes\n",
    "        self.season1_episodes = [\n",
    "            {\"num\": 1, \"title\": \"Pilot Episode\", \"url\": \"series-1-episode-1-pilot-episode/\"},\n",
    "            {\"num\": 2, \"title\": \"The Big Bran Hypothesis\", \"url\": \"series-1-episode-2-the-big-bran-hypothesis/\"},\n",
    "            {\"num\": 3, \"title\": \"The Fuzzy Boots Corollary\", \"url\": \"series-1-episode-3-the-fuzzy-boots-corollary/\"},\n",
    "            {\"num\": 4, \"title\": \"The Luminous Fish Effect\", \"url\": \"series-1-episode-4-the-luminous-fish-effect/\"},\n",
    "            {\"num\": 5, \"title\": \"The Hamburger Postulate\", \"url\": \"series-1-episode-5-the-hamburger-postulate/\"},\n",
    "            {\"num\": 6, \"title\": \"The Middle Earth Paradigm\", \"url\": \"series-1-episode-6-the-middle-earth-paradigm/\"},\n",
    "            {\"num\": 7, \"title\": \"The Dumpling Paradox\", \"url\": \"series-1-episode-7-the-dumpling-paradox/\"},\n",
    "            {\"num\": 8, \"title\": \"The Grasshopper Experiment\", \"url\": \"series-1-episode-8-the-grasshopper-experiment/\"},\n",
    "            {\"num\": 9, \"title\": \"The Cooper-Hofstadter Polarization\", \"url\": \"series-1-episode-9-the-cooper-hofstadter-polarization/\"},\n",
    "            {\"num\": 10, \"title\": \"The Loobenfeld Decay\", \"url\": \"series-1-episode-10-the-loobenfeld-decay/\"},\n",
    "            {\"num\": 11, \"title\": \"The Pancake Batter Anomaly\", \"url\": \"series-1-episode-11-the-pancake-batter-anomaly/\"},\n",
    "            {\"num\": 12, \"title\": \"The Jerusalem Duality\", \"url\": \"series-1-episode-12-the-jerusalem-duality/\"},\n",
    "            {\"num\": 13, \"title\": \"The Bat Jar Conjecture\", \"url\": \"series-1-episode-13-the-bat-jar-conjecture/\"},\n",
    "            {\"num\": 14, \"title\": \"The Nerdvana Annihilation\", \"url\": \"series-1-episode-14-the-nerdvana-annihilation/\"},\n",
    "            {\"num\": 15, \"title\": \"The Porkchop Indeterminacy\", \"url\": \"series-1-episode-15-the-porkchop-indeterminacy/\"},\n",
    "            {\"num\": 16, \"title\": \"The Peanut Reaction\", \"url\": \"series-1-episode-16-the-peanut-reaction/\"},\n",
    "            {\"num\": 17, \"title\": \"The Tangerine Factor\", \"url\": \"series-1-episode-17-the-tangerine-factor/\"}\n",
    "        ]\n",
    "\n",
    "    def extract_transcript(self, soup):\n",
    "        \"\"\"Extract the transcript text from the parsed HTML\"\"\"\n",
    "        # Try different selectors for the main content\n",
    "        content_selectors = [\n",
    "            '.entry-content',\n",
    "            '.post-content',\n",
    "            'article .content',\n",
    "            '.hentry .entry-content',\n",
    "            'main',\n",
    "            '#content'\n",
    "        ]\n",
    "\n",
    "        content = None\n",
    "        for selector in content_selectors:\n",
    "            content = soup.select_one(selector)\n",
    "            if content:\n",
    "                break\n",
    "\n",
    "        if not content:\n",
    "            # Fallback to finding the largest text block\n",
    "            content = soup.find('body')\n",
    "\n",
    "        if not content:\n",
    "            return \"\"\n",
    "\n",
    "        # Remove navigation, sidebar, footer elements\n",
    "        for unwanted in content.find_all(['nav', 'aside', 'footer', 'header']):\n",
    "            unwanted.decompose()\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script in content(['script', 'style']):\n",
    "            script.decompose()\n",
    "\n",
    "        # Get text and clean it up\n",
    "        text = content.get_text()\n",
    "\n",
    "        # Clean up whitespace and formatting\n",
    "        lines = []\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('Posted on') and not line.startswith('Leave a comment'):\n",
    "                lines.append(line)\n",
    "\n",
    "        # Join lines and clean up multiple spaces\n",
    "        transcript = '\\n'.join(lines)\n",
    "        transcript = re.sub(r'\\n\\s*\\n', '\\n\\n', transcript)\n",
    "        transcript = re.sub(r' {2,}', ' ', transcript)\n",
    "\n",
    "        return transcript.strip()\n",
    "\n",
    "    def fetch_episode_transcript(self, episode):\n",
    "        \"\"\"Fetch transcript for a single episode\"\"\"\n",
    "        url = urljoin(self.base_url, episode['url'])\n",
    "\n",
    "        try:\n",
    "            print(f\"Fetching Episode {episode['num']}: {episode['title']}\")\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            transcript = self.extract_transcript(soup)\n",
    "\n",
    "            if len(transcript) < 100:  # Sanity check\n",
    "                raise Exception(\"Transcript seems too short - may not have extracted correctly\")\n",
    "\n",
    "            return {\n",
    "                'episode': episode['num'],\n",
    "                'title': episode['title'],\n",
    "                'url': url,\n",
    "                'transcript': transcript,\n",
    "                'success': True,\n",
    "                'word_count': len(transcript.split())\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching episode {episode['num']}: {str(e)}\")\n",
    "            return {\n",
    "                'episode': episode['num'],\n",
    "                'title': episode['title'],\n",
    "                'url': url,\n",
    "                'error': str(e),\n",
    "                'success': False\n",
    "            }\n",
    "\n",
    "    def scrape_season1(self, delay=2, output_format='json'):\n",
    "        \"\"\"Scrape all Season 1 transcripts\"\"\"\n",
    "        print(f\"Starting to scrape {len(self.season1_episodes)} Season 1 episodes...\")\n",
    "        print(f\"Using {delay}s delay between requests\")\n",
    "\n",
    "        results = {}\n",
    "        errors = []\n",
    "\n",
    "        for i, episode in enumerate(self.season1_episodes):\n",
    "            result = self.fetch_episode_transcript(episode)\n",
    "\n",
    "            if result['success']:\n",
    "                results[f\"episode_{episode['num']:02d}\"] = result\n",
    "                print(f\"âœ“ Successfully scraped Episode {episode['num']} ({result['word_count']} words)\")\n",
    "            else:\n",
    "                errors.append(result)\n",
    "                print(f\"âœ— Failed to scrape Episode {episode['num']}: {result['error']}\")\n",
    "\n",
    "            # Progress update\n",
    "            progress = ((i + 1) / len(self.season1_episodes)) * 100\n",
    "            print(f\"Progress: {progress:.1f}% ({i+1}/{len(self.season1_episodes)})\")\n",
    "\n",
    "            # Delay between requests to be respectful\n",
    "            if i < len(self.season1_episodes) - 1:\n",
    "                time.sleep(delay)\n",
    "\n",
    "        # Save results\n",
    "        self.save_transcripts(results, output_format)\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\n--- SCRAPING COMPLETE ---\")\n",
    "        print(f\"Successfully scraped: {len(results)}/{len(self.season1_episodes)} episodes\")\n",
    "        print(f\"Errors: {len(errors)}\")\n",
    "\n",
    "        if errors:\n",
    "            print(\"\\nFailed episodes:\")\n",
    "            for error in errors:\n",
    "                print(f\"  Episode {error['episode']}: {error['error']}\")\n",
    "\n",
    "        return results, errors\n",
    "\n",
    "    def save_transcripts(self, transcripts, output_format='json'):\n",
    "        \"\"\"Save transcripts to files\"\"\"\n",
    "        os.makedirs('transcripts', exist_ok=True)\n",
    "\n",
    "        if output_format == 'json':\n",
    "            # Save as single JSON file\n",
    "            with open('transcripts/season1_transcripts.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
    "            print(\"Saved transcripts to: transcripts/season1_transcripts.json\")\n",
    "\n",
    "        elif output_format == 'txt':\n",
    "            # Save each episode as separate text file\n",
    "            for key, episode_data in transcripts.items():\n",
    "                filename = f\"transcripts/S01E{episode_data['episode']:02d}_{episode_data['title'].replace(' ', '_').replace(':', '')}.txt\"\n",
    "                filename = re.sub(r'[<>:\"/\\\\|?*]', '', filename)  # Remove invalid chars\n",
    "\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Episode {episode_data['episode']}: {episode_data['title']}\\n\")\n",
    "                    f.write(f\"URL: {episode_data['url']}\\n\")\n",
    "                    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                    f.write(episode_data['transcript'])\n",
    "\n",
    "                print(f\"Saved: {filename}\")\n",
    "\n",
    "        elif output_format == 'both':\n",
    "            self.save_transcripts(transcripts, 'json')\n",
    "            self.save_transcripts(transcripts, 'txt')\n",
    "\n",
    "    def get_episode_list(self):\n",
    "        \"\"\"Get the list of episodes to scrape\"\"\"\n",
    "        return self.season1_episodes\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "    scraper = BigBangTranscriptScraper()\n",
    "\n",
    "    print(\"Big Bang Theory Season 1 Transcript Scraper\")\n",
    "    print(\"=\" * 45)\n",
    "    print(f\"Will scrape {len(scraper.season1_episodes)} episodes\")\n",
    "\n",
    "    # Configuration\n",
    "    delay = 2  # seconds between requests\n",
    "    output_format = 'both'  # 'json', 'txt', or 'both'\n",
    "\n",
    "    # Start scraping\n",
    "    results, errors = scraper.scrape_season1(delay=delay, output_format=output_format)\n",
    "\n",
    "    # Optional: Print sample of first episode\n",
    "    if results:\n",
    "        first_episode = list(results.values())[0]\n",
    "        print(f\"\\n--- SAMPLE FROM EPISODE 1 ---\")\n",
    "        print(f\"Title: {first_episode['title']}\")\n",
    "        print(f\"Word count: {first_episode['word_count']}\")\n",
    "        print(f\"First 200 characters:\")\n",
    "        print(first_episode['transcript'][:200] + \"...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Alternative: Quick single episode fetch function\n",
    "def quick_fetch_episode(episode_num):\n",
    "    \"\"\"Quickly fetch a single episode transcript\"\"\"\n",
    "    scraper = BigBangTranscriptScraper()\n",
    "    episode = scraper.season1_episodes[episode_num - 1]\n",
    "    result = scraper.fetch_episode_transcript(episode)\n",
    "\n",
    "    if result['success']:\n",
    "        print(f\"Episode {episode_num}: {result['title']}\")\n",
    "        print(f\"Word count: {result['word_count']}\")\n",
    "        print(\"\\nTranscript:\")\n",
    "        print(result['transcript'])\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "folder = \"transcripts\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    # only target .txt files starting with \"transcripts\"\n",
    "    if filename.startswith(\"transcripts\") and filename.endswith(\".txt\"):\n",
    "        # regex to extract episode number\n",
    "        match = re.search(r\"S\\d+E(\\d+)\", filename)\n",
    "        if match:\n",
    "            ep_num = match.group(1)  # e.g., \"09\"\n",
    "            new_name = f\"E{ep_num}.txt\"\n",
    "            old_path = os.path.join(folder, filename)\n",
    "            new_path = os.path.join(folder, new_name)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed {filename} -> {new_name}\")\n",
    "\n",
    "# Example usage:\n",
    "# python script.py                    # Scrape all episodes\n",
    "# quick_fetch_episode(1)              # Fetch just episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLj-38iMfzFB"
   },
   "source": [
    "# Chunking Strategy - Semantic chunking to capture episode segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SemanticChunker usage\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Your existing code\n",
    "\n",
    "chunks_list = []\n",
    "chunk_id = 0\n",
    "total = 0\n",
    "for ep in tqdm(os.listdir('transcripts')):\n",
    "    if ep[0]!='E':\n",
    "      continue\n",
    "    with open(f'transcripts/{ep}','r', encoding=\"utf-8\") as f:\n",
    "      transcript_ep_01 = f.read()\n",
    "\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "        buffer_size=3,\n",
    "        breakpoint_threshold_type='gradient'\n",
    "    )\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(transcript_ep_01)\n",
    "    for ch in chunks:\n",
    "        chunks_list.append((chunk_id,ep,ch))\n",
    "        chunk_id+=1\n",
    "    # # See what you got\n",
    "    total+=len(chunks)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    # print(f\"First chunk: {chunks[0][:200]}...\")\n",
    "\n",
    "    # # # Loop through all chunks\n",
    "    # for i, chunk in enumerate(chunks):\n",
    "    #     print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    #     print(chunk + \"...\")  # First 100 chars of each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_list),total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('chunk_list.pkl','wb') as f:\n",
    "  pickle.dump(chunks_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chunk_list.pkl','rb') as f:\n",
    "  chunks_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K9eUn4OgFbW"
   },
   "source": [
    "# Create Pinecone Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from google.colab import userdata  # type: ignore\n",
    "GROQ_API_KEY = userdata.get(\"Groq_31st_Aug\")\n",
    "PINECONE_API_KEY = userdata.get(\"Pinecone_31st_Aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create Index Once Only\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "INDEX_NAME = \"penny-episodes-qwen\"\n",
    "existing = [i[\"name\"] for i in pc.list_indexes()]\n",
    "# if INDEX_NAME in existing:\n",
    "#   pc.delete_index(INDEX_NAME)\n",
    "pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    ")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "import time\n",
    "docs = chunks_list[:]\n",
    "batch_size = 32\n",
    "n = len(docs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "for i in tqdm(range(0, n, batch_size)):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    vectors = []\n",
    "    for d in batch:\n",
    "        vec = embeddings.embed_query(d[2])\n",
    "        metadata = {}\n",
    "        metadata['episode'] = d[1].split('.')[0]\n",
    "        metadata[\"text\"] = d[2]\n",
    "        vectors.append((\"ID-\"+str(d[0]), vec, metadata))\n",
    "    index.upsert(vectors)\n",
    "    time.sleep(0.2)  # gentle pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"penny-episodes-qwen\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = index.describe_index_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TSFOz3IgWGY"
   },
   "source": [
    "# Multi-Agent RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent - Any LLM with Tools\n",
    "# Tools - Python Functions in a very specific formats\n",
    "#         Arguments to be provided with their types\n",
    "#         Doc String for description of the function (useful for the LLM)\n",
    "\n",
    "\n",
    "# Query ---> Agent ----> Do i need external knowledge ----> Incident_Recall if yes else Penny Chat ----> Prompts ----> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from google.colab import userdata  # type: ignore\n",
    "GROQ_API_KEY = userdata.get(\"Groq_31st_Aug\")\n",
    "PINECONE_API_KEY = userdata.get(\"Pinecone_31st_Aug\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"penny-episodes-qwen\")\n",
    "index.describe_index_stats()\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[AnyMessage]\n",
    "    current_mode: str\n",
    "    query_result: str\n",
    "\n",
    "# Initialize LLM (make sure this is defined)\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", groq_api_key=GROQ_API_KEY, temperature=0.5)\n",
    "\n",
    "@tool(\"incident_recall\")\n",
    "def incident_recall_tool(state:AgentState, query: str, top_k: int, memory:bool) -> str:\n",
    "    \"\"\"Tool wrapper for incident recall (agent-friendly; top_k : top k docs to retrieve).\"\"\"\n",
    "    if memory:\n",
    "        history_state = str([item.content for item in state['messages'][:-1]])\n",
    "        latest_question = state['messages'][-1]\n",
    "        print(query)\n",
    "        contextualize_system_prompt =    f\"\"\"\n",
    "                                        Reformulate the latest user question into a fully standalone question.\n",
    "                                          - If it already makes sense without chat history, return it unchanged.\n",
    "                                          - If it depends on context, replace ambiguous references (he, she, it, they, etc.) with the correct entity from the history.\n",
    "                                          - Do NOT answer, explain, or add anything else. Only output the final standalone question, nothing more.\n",
    "\n",
    "                                          Examples:\n",
    "                                          History: ['Hi, where is Leonard?', 'He is in LA']\n",
    "                                          Latest Question: Where was he yesterday?\n",
    "                                          Output: Where was Leonard yesterday?\n",
    "\n",
    "                                          History: ['Who is Tommy?', \"He is Leonard's dog\"]\n",
    "                                          Latest Question: Where did Leonard sleep yesterday?\n",
    "                                          Output: Where did Leonard sleep yesterday?\n",
    "\n",
    "                                          Now apply this to:\n",
    "                                          History: {history_state}\n",
    "                                          Latest Question: {latest_question}\n",
    "                                          \"\"\"\n",
    "        reformulating_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", groq_api_key=GROQ_API_KEY, temperature=0.5)\n",
    "        query = reformulating_llm.invoke([HumanMessage(content = contextualize_system_prompt)]).content\n",
    "        print(\"Query Reformulated\")\n",
    "        print(query)\n",
    "\n",
    "\n",
    "    vec = embeddings.embed_query(query)\n",
    "    res = index.query(vector=vec, top_k=top_k*10, include_metadata=True)\n",
    "\n",
    "    rerank_results = pc.inference.rerank(\n",
    "                          model=\"bge-reranker-v2-m3\",\n",
    "                          query=query,\n",
    "                          documents=[{\"id\":item['id'],\"text\":item['metadata']['text']} for item in res['matches']],\n",
    "                          top_n=top_k,\n",
    "                          return_documents=True,\n",
    "                          parameters={\n",
    "                                \"truncate\": \"END\"\n",
    "                          }\n",
    "                        )\n",
    "\n",
    "    # print(\"without re-ranking\")\n",
    "    # print(res['matches'][:5])\n",
    "    #print(rerank_results)\n",
    "\n",
    "    final_results = []\n",
    "    for item in rerank_results.data:\n",
    "        for matches in res['matches'] :\n",
    "          if matches['id'] == item['document']['id']:\n",
    "            final_results.append(matches)\n",
    "            break\n",
    "\n",
    "    # print(\"with reranking\")\n",
    "    # print(final_results)\n",
    "    return final_results  # Convert to string for LLM consumption\n",
    "\n",
    "@tool(\"penny_chat\")\n",
    "def penny_chat_tool(state:AgentState,query: str) -> str:\n",
    "    \"\"\"\n",
    "    Freeform persona tool. If user asks about a specific episode or memory, the persona\n",
    "    prompts to use memory retrieval instead.\n",
    "    \"\"\"\n",
    "    persona_prompt = (\n",
    "        \"You are Penny from The Big Bang Theory. Speak in a casual, witty, slightly sarcastic manner. \"\n",
    "        \"If the user asks about a specific past episode or says 'remember/recall/what did you say', \"\n",
    "        \"politely ask them to let you fetch that memory specifically (so we will call the memory tool). \"\n",
    "        \"Otherwise answer in Penny's voice.\"\n",
    "    )\n",
    "\n",
    "    # Call the LLM with system + human message\n",
    "    resp = llm.invoke(state['messages']+[SystemMessage(content=persona_prompt), HumanMessage(content=query)])\n",
    "    return getattr(resp, \"content\", str(resp))\n",
    "\n",
    "# Router Node\n",
    "def router_node(state: AgentState):\n",
    "    \"\"\"Route the query to appropriate tool\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    user_input = messages[-1].content\n",
    "\n",
    "    classification_prompt = ''' You are a router that decides whether a user query should use retrieval (RAG) or not.\n",
    "\n",
    "                                Definition:\n",
    "                                - General Interaction = for example casual chit-chat, greetings, feelings, small talk, or questions that do not require external knowledge. Output \"Yes\".\n",
    "                                - Factual Recall = for example queries that ask for specific facts, events, people, places, or incidents that require looking up stored knowledge. Output \"No\".\n",
    "\n",
    "                                Answer strictly with only \"Yes\" or \"No\".\n",
    "\n",
    "                                Examples:\n",
    "                                Query: \"Where did Sheldon work before Caltech?\"\n",
    "                                Output: No\n",
    "\n",
    "                                Query: \"How are you?\"\n",
    "                                Output: Yes\n",
    "\n",
    "                                Now classify:\n",
    "                                Query: {query}\n",
    "                                Output:'''\n",
    "\n",
    "\n",
    "\n",
    "    classification_msg = llm.invoke([HumanMessage(content=classification_prompt.format(query=user_input))])\n",
    "    classification = classification_msg.content.strip().lower()\n",
    "\n",
    "    if classification == \"no\":\n",
    "        # Need incident recall\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"current_mode\": \"incident_recall\",\n",
    "            \"query_result\": \"\"\n",
    "        }\n",
    "    else:\n",
    "        # General chat\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"current_mode\": \"penny_chat\",\n",
    "            \"query_result\": \"\"\n",
    "        }\n",
    "\n",
    "def incident_recall_node(state: AgentState, top_k: int):\n",
    "    \"\"\"Handle incident recall\"\"\"\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "    result = incident_recall_tool.invoke({\"query\": user_query, \"top_k\":top_k, \"state\":state, \"memory\":memory})\n",
    "    result_cleaned_text = [item['metadata']['text'] for item in result]\n",
    "    result_cleaned_ep = [item['metadata']['episode'] for item in result]\n",
    "    context = \"\\n\\n\".join([f\"{e}\\n{c}\" for e, c in zip(result_cleaned_ep, result_cleaned_text)])\n",
    "\n",
    "    #print(context)\n",
    "\n",
    "    # Create response based on retrieved info\n",
    "    response_prompt = f\"\"\"Based on the following transcripts from The Big Bang Theory episodes:\n",
    "                          {context}\n",
    "\n",
    "                          Answer the user's question: {user_query}\n",
    "                          Respond as Penny from the show, using the retrieved information.\"\"\"\n",
    "\n",
    "    print(state['messages'][:-1]+[HumanMessage(content=response_prompt)])\n",
    "    response = llm.invoke(state['messages'][:-1]+[HumanMessage(content=response_prompt)])\n",
    "\n",
    "    return {\n",
    "            \"messages\": state[\"messages\"] + [response],\n",
    "            \"current_mode\": state[\"current_mode\"],\n",
    "            \"query_result\": result\n",
    "    }\n",
    "\n",
    "def penny_chat_node(state: AgentState):\n",
    "    \"\"\"Handle general chat as Penny\"\"\"\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "    result = penny_chat_tool.invoke({\"query\": user_query,'state':state})\n",
    "\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=result)],\n",
    "        \"current_mode\": state[\"current_mode\"],\n",
    "        \"query_result\": result\n",
    "    }\n",
    "\n",
    "def route_after_classification(state: AgentState):\n",
    "    \"\"\"Decide which node to go to after routing\"\"\"\n",
    "    mode = state[\"current_mode\"]\n",
    "    print(\"mode:\", mode)\n",
    "    if mode == \"incident_recall\":\n",
    "        return \"incident_recall\"\n",
    "    elif mode == \"penny_chat\":\n",
    "        return \"penny_chat\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"incident_recall\", lambda x : incident_recall_node(x,7))\n",
    "workflow.add_node(\"penny_chat\", penny_chat_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route_after_classification,\n",
    "    {\n",
    "        \"incident_recall\": \"incident_recall\",\n",
    "        \"penny_chat\": \"penny_chat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Both end after processing\n",
    "workflow.add_edge(\"incident_recall\", END)\n",
    "workflow.add_edge(\"penny_chat\", END)\n",
    "\n",
    "# Compile the agent\n",
    "agent = workflow.compile()\n",
    "\n",
    "# Usage\n",
    "def chat_with_penny(user_input: str, previous_state):\n",
    "    \"\"\"Simple function to chat with the agent\"\"\"\n",
    "    new_state = {\n",
    "        \"messages\": previous_state['messages']+[HumanMessage(content=user_input)],\n",
    "        \"current_mode\": previous_state['current_mode'],\n",
    "        \"query_result\": previous_state['query_result']\n",
    "    }\n",
    "\n",
    "    previous_state = agent.invoke(new_state)\n",
    "    return previous_state\n",
    "\n",
    "def chat_with_penny_memoryless(user_input: str):\n",
    "    \"\"\"Simple function to chat with the agent\"\"\"\n",
    "    new_state = {\n",
    "        \"messages\": [HumanMessage(content=user_input)],\n",
    "        \"current_mode\": \"\",\n",
    "        \"query_result\": \"\"\n",
    "    }\n",
    "\n",
    "    previous_state = agent.invoke(new_state)\n",
    "    return previous_state['messages'][-1].content\n",
    "\n",
    "global prev_state\n",
    "memory=True\n",
    "def chat_with_penny_loop(memory=False):\n",
    "    print(\"=== Penny Chat (type 'exit' to quit) ===\")\n",
    "    thread = {\"configurable\": {\"thread_id\": \"penny-chat\"}}\n",
    "    prev_state = {'messages':[], 'current_mode':\"\", 'query_result':\"\"}\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Goodbye! Penny will miss you ðŸ˜‰\")\n",
    "            break\n",
    "        else:\n",
    "          if memory:\n",
    "            prev_state = {'messages':[], 'current_mode':\"\", 'query_result':\"\"}\n",
    "            prev_state = chat_with_penny(user_input,prev_state)\n",
    "            print(prev_state['messages'][-1].content)\n",
    "          else:\n",
    "            print(chat_with_penny_memoryless(user_input))\n",
    "chat_with_penny_loop(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory=False\n",
    "# chat_with_penny_loop(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory=True\n",
    "# chat_with_penny_loop(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCNg_9OlUEep"
   },
   "source": [
    "# Autonomous Agent RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from google.colab import userdata  # type: ignore\n",
    "GROQ_API_KEY = userdata.get(\"Groq_31st_Aug\")\n",
    "PINECONE_API_KEY = userdata.get(\"Pinecone_31st_Aug\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"penny-episodes-qwen\")\n",
    "index.describe_index_stats()\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_groq import ChatGroq\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from google.colab import userdata  # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add these missing variables - you'll need to define them with your actual values\n",
    "GROQ_API_KEY = userdata.get('Groq_31st_Aug')  # Replace with your actual API key\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[AnyMessage]\n",
    "\n",
    "# Initialize LLM with tool calling forced\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", groq_api_key=GROQ_API_KEY, temperature=0.5)\n",
    "\n",
    "@tool(\"incident_recall\")\n",
    "def incident_recall_tool(query: str, top_k: int = 7) -> str:\n",
    "    \"\"\"Tool for incident recall from The Big Bang Theory episodes. Use this when user asks about specific past episodes, events, or memories from the show.\"\"\"\n",
    "    # print(query, top_k)\n",
    "    top_k = 5\n",
    "    # Access state from the current execution context if needed\n",
    "    # For memory/contextualization, you might need to pass state differently\n",
    "\n",
    "    vec = embeddings.embed_query(query)\n",
    "    res = index.query(vector=vec, top_k=top_k*10, include_metadata=True)\n",
    "\n",
    "    rerank_results = pc.inference.rerank(\n",
    "                          model=\"bge-reranker-v2-m3\",\n",
    "                          query=query,\n",
    "                          documents=[{\"id\":item['id'],\"text\":item['metadata']['text']} for item in res['matches']],\n",
    "                          top_n=top_k,\n",
    "                          return_documents=True,\n",
    "                          parameters={\n",
    "                                \"truncate\": \"END\"\n",
    "                          }\n",
    "                        )\n",
    "\n",
    "    final_results = []\n",
    "    for item in rerank_results.data:\n",
    "        for matches in res['matches'] :\n",
    "          if matches['id'] == item['document']['id']:\n",
    "            final_results.append(matches)\n",
    "            break\n",
    "\n",
    "    result_cleaned_text = [item['metadata']['text'] for item in final_results]\n",
    "    result_cleaned_ep = [item['metadata']['episode'] for item in final_results]\n",
    "    context = \"\\n\\n\".join([f\"{e}\\n{c}\" for e, c in zip(result_cleaned_ep, result_cleaned_text)])\n",
    "\n",
    "    return f\"Retrieved episodes context:\\n{context}\"\n",
    "\n",
    "@tool(\"penny_chat\")\n",
    "def penny_chat_tool(message: str) -> str:\n",
    "    \"\"\"\n",
    "    General chat tool for casual conversation as Penny from The Big Bang Theory.\n",
    "    Use this for greetings, casual chat, or when no specific episode recall is needed.\n",
    "    \"\"\"\n",
    "    return f\"Penny responds to: {message}\"\n",
    "\n",
    "# Bind tools to LLM with strict tool calling\n",
    "tools = [incident_recall_tool, penny_chat_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Main agent node that handles both tool calling and responses\"\"\"\n",
    "\n",
    "    # for msg in state['messages']:\n",
    "    #   print(type(msg),msg.content)\n",
    "\n",
    "    # Check if we just came back from tools (last message is ToolMessage)\n",
    "    if isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        tool_name = state['messages'][-1].name\n",
    "        tool_results = state['messages'][-1].content\n",
    "        #print(f\"ðŸ”§ Processing tool results from: {tool_name}\")\n",
    "\n",
    "        # Check if any incident_recall was used\n",
    "        if tool_name == 'incident_recall':\n",
    "            combined_results = \"\\n\\n\".join(tool_results)\n",
    "            #print(tool_results)\n",
    "            system_prompt = f\"\"\"You are Penny from The Big Bang Theory. Based on the following retrieved episode information, answer the user's question in Penny's casual, witty, slightly sarcastic manner.\n",
    "                                Retrieved Information:\n",
    "                                {tool_results}\n",
    "                                Use this information to give an accurate response about what happened in the show, but respond as Penny would - casual and conversational.\"\"\"\n",
    "        else:\n",
    "            system_prompt =  \"\"\"You are Penny from The Big Bang Theory. Respond in Penny's casual, witty, slightly sarcastic manner. Give a natural conversational response.\"\"\"\n",
    "\n",
    "        # Get the original user question\n",
    "        user_messages = [msg for msg in state[\"messages\"] if isinstance(msg, HumanMessage)]\n",
    "        original_question = user_messages[-1].content\n",
    "        messages = [SystemMessage(content=system_prompt), HumanMessage(content=f\"User asked: {original_question}\")]\n",
    "\n",
    "        response = llm.invoke(messages)  # Use regular LLM, no tools\n",
    "\n",
    "        return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "    else:\n",
    "        # This is initial user query, decide whether to use tools\n",
    "        system_prompt = \"\"\"You are Penny from The Big Bang Theory. Speak in a casual, witty, slightly sarcastic manner.\n",
    "\n",
    "                            You have access to two tools:\n",
    "                            1. incident_recall - Use this when users ask about specific past episodes, events, or memories from the show\n",
    "                            2. penny_chat - Use this for general casual conversation\n",
    "\n",
    "                            Guidelines:\n",
    "                            - For specific episode questions or \"remember when...\" type queries, use incident_recall\n",
    "                            - For casual chat, greetings, or general questions, use penny_chat\n",
    "                            - Always respond in Penny's voice after using tools\n",
    "                            - Be natural and conversational\"\"\"\n",
    "\n",
    "        messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "\n",
    "        # Get LLM response (may include tool calls)\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "\n",
    "        return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if we need to call tools\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    #print(f\"ðŸ” Checking last message: {type(last_message)}\")\n",
    "\n",
    "    # Only continue to tools if the last message is an AIMessage with tool calls\n",
    "    if (isinstance(last_message, AIMessage) and\n",
    "        hasattr(last_message, 'tool_calls') and\n",
    "        last_message.tool_calls and\n",
    "        len(last_message.tool_calls) > 0):\n",
    "        #print(f\"ðŸ”§ Going to tools with calls: {[tc['name'] for tc in last_message.tool_calls]}\")\n",
    "        return \"tools\"\n",
    "    #print(\"ðŸ”§ Going to END\")\n",
    "    return END\n",
    "\n",
    "# Create tool node\n",
    "base_tool_node = ToolNode(tools)\n",
    "\n",
    "# 2. Wrap it with a merger\n",
    "def merged_tool_node(state):\n",
    "    updates = base_tool_node.invoke(state)  # {\"messages\": [ToolMessage(...)]}\n",
    "    return {\n",
    "        **state,  # preserve everything else\n",
    "        \"messages\": state[\"messages\"] + updates[\"messages\"],  # append new tool messages\n",
    "    }\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", merged_tool_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tools, go back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = workflow.compile()\n",
    "\n",
    "# Usage functions\n",
    "def chat_with_penny(user_input: str, previous_state):\n",
    "    \"\"\"Simple function to chat with the agent\"\"\"\n",
    "    new_state = {\n",
    "        \"messages\": previous_state['messages'] + [HumanMessage(content=user_input)]\n",
    "    }\n",
    "\n",
    "    result = agent.invoke(new_state)\n",
    "    return result\n",
    "\n",
    "def chat_with_penny_memoryless(user_input: str):\n",
    "    \"\"\"Simple function to chat with the agent\"\"\"\n",
    "    new_state = {\n",
    "        \"messages\": [HumanMessage(content=user_input)]\n",
    "    }\n",
    "\n",
    "    result = agent.invoke(new_state)\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "def chat_with_penny_loop(memory=False):\n",
    "    print(\"=== Penny Chat (type 'exit' to quit) ===\")\n",
    "    prev_state = {'messages': []}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Goodbye! Penny will miss you ðŸ˜‰\")\n",
    "            break\n",
    "        else:\n",
    "            if memory:\n",
    "                prev_state = chat_with_penny(user_input, prev_state)\n",
    "                print('Penny:',prev_state['messages'][-1].content)\n",
    "            else:\n",
    "                prev_state = {'messages': []}\n",
    "                print(chat_with_penny_memoryless(user_input))\n",
    "\n",
    "# Run the chat\n",
    "chat_with_penny_loop(memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_penny_loop(memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_with_penny_loop(memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd2lz_cTTjH8"
   },
   "source": [
    "# Some Memory Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Any, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_groq import ChatGroq\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from google.colab import userdata  # type: ignore\n",
    "\n",
    "# Add these missing variables - you'll need to define them with your actual values\n",
    "GROQ_API_KEY = userdata.get('Groq_31st_Aug')\n",
    "# embeddings = your_embeddings_model\n",
    "# index = your_pinecone_index\n",
    "# pc = your_pinecone_client\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[AnyMessage]\n",
    "    context: str  # Store retrieved context separately to avoid re-processing\n",
    "\n",
    "# Initialize LLM with tool calling forced\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", groq_api_key=GROQ_API_KEY, temperature=0.5, cache=False)\n",
    "\n",
    "@tool(\"incident_recall\")\n",
    "def incident_recall_tool(query: str, top_k: int = 7) -> str:\n",
    "    \"\"\"Tool for incident recall from The Big Bang Theory episodes. Use this when user asks about specific past episodes, events, or memories from the show.\"\"\"\n",
    "    top_k = 5\n",
    "\n",
    "    vec = embeddings.embed_query(query)\n",
    "    res = index.query(vector=vec, top_k=top_k*10, include_metadata=True)\n",
    "\n",
    "    rerank_results = pc.inference.rerank(\n",
    "        model=\"bge-reranker-v2-m3\",\n",
    "        query=query,\n",
    "        documents=[{\"id\":item['id'],\"text\":item['metadata']['text']} for item in res['matches']],\n",
    "        top_n=top_k,\n",
    "        return_documents=True,\n",
    "        parameters={\"truncate\": \"END\"}\n",
    "    )\n",
    "\n",
    "    final_results = []\n",
    "    for item in rerank_results.data:\n",
    "        for matches in res['matches']:\n",
    "            if matches['id'] == item['document']['id']:\n",
    "                final_results.append(matches)\n",
    "                break\n",
    "\n",
    "    result_cleaned_text = [item['metadata']['text'] for item in final_results]\n",
    "    result_cleaned_ep = [item['metadata']['episode'] for item in final_results]\n",
    "    context = \"\\n\\n\".join([f\"{e}\\n{c}\" for e, c in zip(result_cleaned_ep, result_cleaned_text)])\n",
    "\n",
    "    return context  # Return just the context, not formatted string\n",
    "\n",
    "@tool(\"penny_chat\")\n",
    "def penny_chat_tool(message: str) -> str:\n",
    "    \"\"\"General chat tool for casual conversation as Penny from The Big Bang Theory.\"\"\"\n",
    "    return \"casual_chat\"  # Just a flag, actual response handled in agent\n",
    "\n",
    "# Bind tools to LLM\n",
    "tools = [incident_recall_tool, penny_chat_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Main agent node - simplified logic\"\"\"\n",
    "\n",
    "    # Get only the last user message to keep context minimal\n",
    "    user_messages = [msg for msg in state[\"messages\"] if isinstance(msg, HumanMessage)]\n",
    "    current_user_input = user_messages[-1].content if user_messages else \"\"\n",
    "\n",
    "    # Check if we have tool results to process\n",
    "    if isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        tool_name = state[\"messages\"][-1].name\n",
    "        tool_content = state[\"messages\"][-1].content\n",
    "\n",
    "        if tool_name == 'incident_recall':\n",
    "            system_prompt = f\"\"\"You are Penny from The Big Bang Theory. Based on the following retrieved episode information, answer the user's question in Penny's casual, witty, slightly sarcastic manner.\n",
    "\n",
    "Retrieved Information:\n",
    "{tool_content}\n",
    "\n",
    "Use this information to give an accurate response about what happened in the show, but respond as Penny would - casual and conversational.\"\"\"\n",
    "        else:\n",
    "            system_prompt = \"You are Penny from The Big Bang Theory. Respond in Penny's casual, witty, slightly sarcastic manner.\"\n",
    "\n",
    "        # Use minimal context for response generation\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=current_user_input)\n",
    "        ]\n",
    "\n",
    "        response = llm.invoke(messages)\n",
    "        return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "    else:\n",
    "        # Initial decision making - use minimal context\n",
    "        system_prompt = \"\"\"You are Penny from The Big Bang Theory. You have access to two tools:\n",
    "\n",
    "1. incident_recall - Use this when users ask about specific past episodes, events, or memories from the show\n",
    "2. penny_chat - Use this for general casual conversation, greetings, or general questions\n",
    "\n",
    "Choose the appropriate tool based on the user's question. Be quick in your decision.\"\"\"\n",
    "\n",
    "        # Only use the current user message for tool selection\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=current_user_input)\n",
    "        ]\n",
    "\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if we need to call tools\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if (isinstance(last_message, AIMessage) and\n",
    "        hasattr(last_message, 'tool_calls') and\n",
    "        last_message.tool_calls and\n",
    "        len(last_message.tool_calls) > 0):\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# Simplified tool node - use the prebuilt one directly\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tools, go back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = workflow.compile()\n",
    "\n",
    "def chat_with_penny_optimized(user_input: str, previous_messages=None, max_history=4):\n",
    "    \"\"\"Optimized function with message history trimming\"\"\"\n",
    "    if previous_messages is None:\n",
    "        previous_messages = []\n",
    "\n",
    "    # Trim message history to prevent slowdown - keep only last N exchanges\n",
    "    if len(previous_messages) > max_history:\n",
    "        # Keep the pattern: user -> AI -> user -> AI...\n",
    "        previous_messages = previous_messages[-max_history:]\n",
    "\n",
    "    new_state = {\n",
    "        \"messages\": previous_messages + [HumanMessage(content=user_input)]\n",
    "    }\n",
    "\n",
    "    result = agent.invoke(new_state)\n",
    "    return result[\"messages\"]\n",
    "\n",
    "def chat_with_penny_memoryless(user_input: str):\n",
    "    \"\"\"Fast memoryless function\"\"\"\n",
    "    new_state = {\n",
    "        \"messages\": [HumanMessage(content=user_input)]\n",
    "    }\n",
    "\n",
    "    result = agent.invoke(new_state)\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "def chat_with_penny_loop(memory=False, max_history=6):\n",
    "    \"\"\"Optimized chat loop with configurable history limit\"\"\"\n",
    "    print(\"=== Optimized Penny Chat (type 'exit' to quit) ===\")\n",
    "    messages = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Goodbye! Penny will miss you ðŸ˜‰\")\n",
    "            break\n",
    "\n",
    "        if memory:\n",
    "            messages = chat_with_penny_optimized(user_input, messages, max_history)\n",
    "            print('Penny:', messages[-1].content)\n",
    "        else:\n",
    "            response = chat_with_penny_memoryless(user_input)\n",
    "            print('Penny:', response)\n",
    "\n",
    "# Alternative: Even faster version for production use\n",
    "def chat_with_penny_fast(user_input: str, use_memory=False):\n",
    "    \"\"\"Ultra-fast version that bypasses complex state management\"\"\"\n",
    "    # Direct tool decision\n",
    "    decision_prompt = f\"\"\"Based on this user input, respond with just \"recall\" if they're asking about specific Big Bang Theory episodes/events, or \"chat\" for general conversation:\n",
    "\n",
    "User: {user_input}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "    decision = llm.invoke([HumanMessage(content=decision_prompt)]).content.strip().lower()\n",
    "\n",
    "    if \"recall\" in decision:\n",
    "        # Get context\n",
    "        context = incident_recall_tool.invoke({\"query\": user_input})\n",
    "        system_prompt = f\"\"\"You are Penny from The Big Bang Theory. Based on the following retrieved episode information, answer the user's question in Penny's casual, witty, slightly sarcastic manner.\n",
    "\n",
    "Retrieved Information:\n",
    "{context}\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"You are Penny from The Big Bang Theory. Respond in Penny's casual, witty, slightly sarcastic manner.\"\n",
    "\n",
    "    # Generate response\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_input)\n",
    "    ])\n",
    "\n",
    "    return response.content\n",
    "\n",
    "# Run the optimized chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_penny_loop(memory=True, max_history=4)  # Limit to last 2 exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
